# 第一次作业

- 位置：在2-3的第31分钟
- 提交作业邮箱：huangguanglun14@mails.ucas.edu.cn
- 实现Q-learning算法。

![image-20200525231716499](%E7%AC%AC%E4%B8%80%E6%AC%A1%E4%BD%9C%E4%B8%9A.assets/image-20200525231716499.png)![image-20200525232553348](%E7%AC%AC%E4%B8%80%E6%AC%A1%E4%BD%9C%E4%B8%9A.assets/image-20200525232553348.png)

## 安装环境

- 安装gym：https://github.com/openai/gym

~~~shell
pip install gym
~~~

### gym说明

- 导入Gym库之后，可以通过 make()函数来得到环境对象，每个环境都有一个ID，格式"Xxxx-vd"，d表示版本号，例如：

~~~python
env = gym.make('CartPole-v0')
~~~

- 查看Gym库已经注册了哪些环境

~~~python
from gym import envs
env_specs = envs.registry.all()
env_ids = [env_spec.id for env_spec in env_specs]
env_ids
~~~

- 每个环境都定义了自己的观测空间和动作空间。环境env的观测空间用env.observation_space表示，动作空间用env.action_space 表示。观测空间分为离散空间和，表示为gym.spaces.Discrete，和连续空间，表示为gym.spaces.Box。例如，环境‘MountainCar-v0'的观测空间是Box（2），表示观测可以用两个float值表示，而动作空间是Discrete(3)，表示动作取值{1，2，3}。

- 接下来使用环境对象env，首先初始化环境对象env

~~~python
env.reset()
~~~

- 该调用能返回智能体的初始观测，是np.array对象，环境初始化之后就可以使用了。

- 使用环境的核心是使用环境对象的step()方法，接收智能体的动作作为参数，并返回以下4个参数。

1. observation：np.array对象，表示观测，和env.reset()的返回值意义相同。

2. reward : float类型的值。

3. done:  bool类型的数值，本回合结束提示。Gym库里的实验环境大多是回合制的，这个返回值可以指示当前动作后游戏是否结束，如果结束，可以通过env.reset()开始下一回合。

4. info : dict类型的值，其他信息，包含以下调试信息。

- env.reset()的参数取自动作空间。可以使用以下语句从动作空间中随机选取一个动作：

~~~python
action = env.action_space.sample()
~~~

- 每次调用env.step()只会让环境前进一步。所以env.step()往往放在循环结构里面，通过循环调用来完成整个回合。

- 在env.reset()和env.step()后，可以用以下语句以图形化的方法显示当前环境。

~~~python
env.render()
~~~

- 使用完环境后，可以用下列语句关闭环境。注意：强行关闭图形界面可能导致死机，最好用以下命令关闭。

~~~python
env.close()
~~~

