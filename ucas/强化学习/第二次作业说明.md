# 第二次作业

- Advantage-Actor-Critic（A2C）
- 视频4-3最后。ppt第六章69页。

## A2C 算法

- A代表Advantage学习和Actor-Critic。在A2C中，所有agent共享相同的深度神经网络。
- Advantage学习用两步以上而不是一步来更新Q函数。通常需要使用具有合理部署的Advantage学习。
- Actor-Critic同时使用了策略迭代法和价值迭代法。
- 输入：输入是一个状态，是CartPole的位置、速度、角度和角速度四个变量。
- 输出：两个元素Actor与Critic。
    - Actor具有与动作类型数量一样多的输出。CartPole中，输出长度为2。
    - Critic的输出表示状态值。

## CartPole

- 原文链接：https://blog.csdn.net/qq_32892383/java/article/details/89576003
- Cart Pole即车杆游戏，游戏模型如下图所示。游戏里面有一个小车，上有竖着一根杆子，每次重置后的初始状态会有所不同。小车需要左右移动来保持杆子竖直，为了保证游戏继续进行需要满足以下两个条件：
- 杆子倾斜的角度θ\thetaθ不能大于15°
- 小车移动的位置xxx需保持在一定范围（中间到两边各2.4个单位长度）
- ![image-20200612222456130](%E7%AC%AC%E4%BA%8C%E6%AC%A1%E4%BD%9C%E4%B8%9A%E8%AF%B4%E6%98%8E.assets/image-20200612222456130.png)
- ![image-20200612222519838](%E7%AC%AC%E4%BA%8C%E6%AC%A1%E4%BD%9C%E4%B8%9A%E8%AF%B4%E6%98%8E.assets/image-20200612222519838.png)
