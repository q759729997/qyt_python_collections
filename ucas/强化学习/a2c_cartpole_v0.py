# -*- coding: utf-8 -*-
"""a2c_CartPole-V0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NlOtvH5ldQzigZgr8u6ZwwXCvOO9moJY

#车杆平衡问题:CartPole-V0

- 问题描述：一个小车可以在直线滑轨上移动，一个杆子一头连着小车，另外一头悬空，可以不完全智力。小车的初始位置和杆子的初始角度都是在一定范围内随机选取的。
- 智能体可以控制小车沿着滑轨左移一个单位或者右移1段固定长度，不可以不移动。
- 出现以下情形中的任一情形时，回合结束：
  - 杆子的倾斜角超过12°；
  - 小车移动超过2.4个单位长度；
  - 一个回合内达到200步；
- 每进行一步得到一个单位的奖励，希望回合尽量长。目标希望100个回合平均奖励超过195.
"""

# 日志初始化
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
logging.info('logging test')

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import gym
import numpy as np

env = gym.make('CartPole-v0')
logging.info('观测空间：{}'.format(env.observation_space))
logging.info('动作空间：{}'.format(env.action_space))
logging.info('观测范围：{}至{}'.format(env.observation_space.low, env.observation_space.high))
logging.info('动作数：{}'.format(env.action_space.n))

"""- 观察值有4个分量，分别是小车位置、小车速度、杆子角度、杆子角速度。
- 动作取值为0或1，分别表示向左和向右。

## 随机策略
"""

def play(env, render=False):
    episode_reward = 0
    observation = env.reset()
    step = 0
    while True:
        if render:
            env.render()
        action = env.action_space.sample()
        next_observation, reward, done, _ = env.step(action)
        
        episode_reward += reward
        if done:
            break
        step += 1
        observation = next_observation
    return episode_reward

# 测试
logging.info('使用随机策略进行测试')
episode_rewards = [play(env, render=False) for _ in range(100)]
logging.info('平均奖励为{}/{}={}'.format(sum(episode_rewards), len(episode_rewards), np.mean(episode_rewards)))

"""## A2C算法
- 优势执行者/评论者算法（Advantage Actor-Critic，A2C）
"""

import matplotlib.pyplot as plt

import tensorflow.compat.v2 as tf
from tensorflow import keras

class RewardChart:
    """训练时奖励变化绘图工具类"""
    def __init__(self):
        self.fig, self.ax = plt.subplots(1, 1)

    def plot(self, episode_rewards):
        self.ax.clear()
        self.ax.plot(episode_rewards)
        self.ax.set_xlabel('iteration')
        self.ax.set_ylabel('episode reward')
        self.fig.canvas.draw()

# 输入是一个状态，是CartPole的位置、速度、角度和角速度四个变量
observation = env.reset()
# np.newaxis 在使用和功能上等价于 None
input_shape = tf.convert_to_tensor(observation[np.newaxis], dtype=tf.float32).get_shape()
logging.info('input_shape:{}'.format(input_shape))

# 构造执行者网络
output_size = env.action_space.n  # 输出为动作空间
logging.info('output_size:{}'.format(output_size))
actor_net = keras.Sequential()
# A linear layer
actor_net.add(keras.layers.Dense(units=32, activation=tf.nn.relu))
# actor_net.add(keras.layers.Dense(units=16, activation=tf.nn.relu))
actor_net.add(keras.layers.Dense(units=output_size, activation=tf.nn.softmax))
optimizer = tf.optimizers.Adam(0.001)  # 0.001 设置学习率
# 使用交叉熵计算loss
actor_net.compile(optimizer=optimizer, loss=tf.losses.categorical_crossentropy)
actor_net.build(input_shape)
actor_net.summary()

# 构造评论者网络
output_size = 1  # 输出为状态价值
logging.info('output_size:{}'.format(output_size))
critic_net = keras.Sequential()
# A linear layer
critic_net.add(keras.layers.Dense(units=32, activation=tf.nn.relu))
# critic_net.add(keras.layers.Dense(units=16, activation=tf.nn.relu))
critic_net.add(keras.layers.Dense(units=output_size, activation=None))
optimizer = tf.optimizers.Adam(0.002)  # 0.002 设置学习率
# 使用mse计算loss
critic_net.compile(optimizer=optimizer, loss=tf.losses.mse)
critic_net.build(input_shape)
critic_net.summary()

class AdvantageActorCriticAgent(object):
    """A2C智能体"""
    def __init__(self, gamma=0.99):
        self.action_n = env.action_space.n
        self.gamma = gamma
        self.discount = 1.

    def decide(self, observation):
        """actor网络对observation进行预测，按概率求取下一步动作"""
        probs = actor_net.predict(observation[np.newaxis])[0]
        action = np.random.choice(self.action_n, p=probs)
        return action

    def learn(self, observation, action, reward, next_observation, done):
        x = observation[np.newaxis]
        next_x = next_observation[np.newaxis]
        next_x_tensor = tf.convert_to_tensor(next_x, dtype=tf.float32)
        # 使用critic网络计算observation与next_observation状态价值
        u = reward + (1. - done) * self.gamma * critic_net.predict(next_x_tensor)
        td_error = u - critic_net.predict(x)

        # 训练执行者网络
        x_tensor = tf.convert_to_tensor(observation[np.newaxis], dtype=tf.float32)
        # 使用梯度带计算梯度
        with tf.GradientTape() as tape:
            pi_tensor = actor_net(x_tensor)[0, action]
            logpi_tensor = tf.math.log(tf.clip_by_value(pi_tensor, 1e-6, 1.))
            loss_tensor = -self.discount * td_error * logpi_tensor
        grad_tensors = tape.gradient(loss_tensor, actor_net.variables)
        # 更新执行者网络
        actor_net.optimizer.apply_gradients(zip(grad_tensors, actor_net.variables))

        # 训练评论者网络
        critic_net.fit(x, u, verbose=0)  # 更新评论者网络

        if done:
            self.discount = 1.  # 为下一回合初始化累积折扣
        else:
            self.discount *= self.gamma  # 进一步累积折扣

def play_a2c(env, agent, train=False, render=False):
  episode_reward = 0
  observation = env.reset()
  step = 0
  while True:
    if render:
        env.render()
    action = agent.decide(observation)
    next_observation, reward, done, _ = env.step(action)
    episode_reward += reward
    if train:
        agent.learn(observation, action, reward, next_observation, done)
    if done:
        break
    step += 1
    observation = next_observation
  return episode_reward

# 训练10轮
all_episodes = 0
logging.info('训练10轮')
episodes = 10
episode_rewards = []
chart = RewardChart()
agent = AdvantageActorCriticAgent()
start_episodes = all_episodes
stop_episodes = episodes + all_episodes
for episode in range(start_episodes, stop_episodes):
  all_episodes += 1
  episode_reward = play_a2c(env, agent, train=True)
  episode_rewards.append(episode_reward)
  logging.info('episode:{}， episode_reward：{}, average_reward:{}'.format(all_episodes, episode_reward, np.mean(episode_rewards)))
chart.plot(episode_rewards)
# 测试
episode_rewards = [play_a2c(env, agent) for _ in range(100)]
logging.info('平均奖励为{}/{}={}'.format(sum(episode_rewards), len(episode_rewards), np.mean(episode_rewards)))

logging.info('再训练50轮')
episodes = 50
# episode_rewards = []
chart = RewardChart()
# agent = AdvantageActorCriticAgent()
start_episodes = all_episodes
stop_episodes = episodes + all_episodes
for episode in range(start_episodes, stop_episodes):
  all_episodes += 1
  episode_reward = play_a2c(env, agent, train=True)
  episode_rewards.append(episode_reward)
  if all_episodes % 10 == 0:
    logging.info('episode:{}， episode_reward：{}, average_reward:{}'.format(all_episodes, episode_reward, np.mean(episode_rewards)))
chart.plot(episode_rewards)
# 测试
episode_rewards = [play_a2c(env, agent) for _ in range(100)]
logging.info('平均奖励为{}/{}={}'.format(sum(episode_rewards), len(episode_rewards), np.mean(episode_rewards)))

logging.info('再训练100轮')
episodes = 100
# episode_rewards = []
chart = RewardChart()
# agent = AdvantageActorCriticAgent()
start_episodes = all_episodes
stop_episodes = episodes + all_episodes
for episode in range(start_episodes, stop_episodes):
  all_episodes += 1
  episode_reward = play_a2c(env, agent, train=True)
  episode_rewards.append(episode_reward)
  if all_episodes % 10 == 0:
    logging.info('episode:{}， episode_reward：{}, average_reward:{}'.format(all_episodes, episode_reward, np.mean(episode_rewards)))
chart.plot(episode_rewards)
# 测试
episode_rewards = [play_a2c(env, agent) for _ in range(100)]
logging.info('平均奖励为{}/{}={}'.format(sum(episode_rewards), len(episode_rewards), np.mean(episode_rewards)))

logging.info('再训练200轮')
episodes = 200
# episode_rewards = []
chart = RewardChart()
# agent = AdvantageActorCriticAgent()
start_episodes = all_episodes
stop_episodes = episodes + all_episodes
for episode in range(start_episodes, stop_episodes):
  all_episodes += 1
  episode_reward = play_a2c(env, agent, train=True)
  episode_rewards.append(episode_reward)
  if all_episodes % 10 == 0:
    logging.info('episode:{}， episode_reward：{}, average_reward:{}'.format(all_episodes, episode_reward, np.mean(episode_rewards)))
chart.plot(episode_rewards)
# 测试
episode_rewards = [play_a2c(env, agent) for _ in range(100)]
logging.info('平均奖励为{}/{}={}'.format(sum(episode_rewards), len(episode_rewards), np.mean(episode_rewards)))

"""370多轮训练后，目标为100个回合平均奖励超过195；模型结果为198.54，已到达预期目标。"""

# env.close()